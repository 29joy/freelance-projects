# 星星充电（Part 2）半自动抓取可行性 Checklist

## 0) 基础准备（30–60 分钟）
* **目标**：保证数据口径与产物结构清晰，减少返工。
* 动作：

  * 明确客户需要的字段集合（若表格模板无列名，则自定一版最小字段集）：

    * 基础：`StationName, Operator, Province, City, District, Address, Lat, Lng`
    * 设备：`TruckBays(重卡车位数), DC_Power(kW), ConnectorType, Swap(是否换电), OpenHours`
    * 资费：`Price, ServiceFee, PricingNotes`
    * 可用性：`Status(在营/施工/下线), LastUpdated(最后核验日期)`
    * 证据：`Source(App/WeChat/Web), SourcePath/URL, ScreenshotPath`
    * 其他：`Notes`
  * 定义**命名规范**（省/市/运营商 → Excel 工作表或CSV分片；截图文件名规则）。
* 成功判据：有一份清晰的**字段清单 + 文件命名规范**，可以直接套用到人工或半自动两条路线。

---

## 1) App 端“全国可见性 & 筛选”验证（15–30 分钟）

* **目标**：确认能浏览全国并精准筛选“重卡”站点。
* 动作：

  * 在手机或模拟器中，切换不同省市；验证地图/列表是否可浏览全国站点。
  * 使用“**重卡车位**”或“**重卡**”相关筛选项，确认筛选能生效。
* 成功判据：可**稳定查看全国** + **能筛选重卡**；可进入站点详情页查看字段。

---

## 2) 半自动路线A：抓包找接口（优先）（1–4 小时）

* **目标**：判断是否存在可复用的结构化接口（JSON）。
* 工具选项：

  * 手机 + 代理抓包：**mitmproxy/Charles/Fiddler**
  * Android 模拟器：**BlueStacks/夜神/Android Studio Emulator** + 抓包工具
* 动作：

  1. 在列表页、详情页进行操作，抓取请求：

     * 关注`/search/list`、`/poi`、`/station/detail`、地图瓦片请求背后的**业务接口**；
     * 看 **返回是否 JSON**、是否含**坐标/城市/资费/是否重卡**等字段；
     * 注意是否存在 **token/签名/时间戳**（若有，评估重放难度）。
  2. 导出少量样本 JSON（20–50 条），验证字段能否映射到你的字段清单。
* 成功判据：

  * ✅ 抓到结构化 JSON，字段可映射；或
  * ❌ 发现强加密/SSL Pinning/签名不可绕过 → 转路线B或C。
* 注意：

  * 遵守 ToS/法律合规；仅用于**客户授权的数据调研**；
  * 如果存在 **SSL Pinning**，可以尝试模拟器低版本或特定抓包配置；不可逆时**放弃**，避免浪费时间。

---

## 3) 半自动路线B：H5/内嵌页面提取（1–2 小时）

* **目标**：判断 App 内是否嵌 H5 页面（URL 可复用）。
* 动作：

  * 观察站点详情是否会打开内置浏览器/H5（链接复制能力、分享功能、webview痕迹）。
  * 若存在网页端入口，尝试在 PC 端复现，检查是否能分页拉取或组合查询。
  * 如果能拿到 URL 参数（城市/经纬度），可用 Python + requests 半自动拉取。
* 成功判据：找到 **可参数化** 的 H5 请求路径或可复用 URL；否则转路线C。

---

## 4) 半自动路线C：截图 + OCR（2–6 小时试验）

* **目标**：即使无接口，也借助**批量截图 + OCR**提速。
* 工具：

  * **Appium/ADB** 自动化操作（翻页、点击详情 → 自动截图）；
  * OCR：**Tesseract**（本地）、或云端 OCR（百度/讯飞等，需考虑成本）；
  * Python 清洗：正则抽取字段，生成结构化表格。
* 动作：

  1. 选择 1–2 个省做试验（约 100–200 条），跑一轮 **自动翻页截图 → OCR → 解析**；
  2. 评估准确率（地址、功率、价格、是否重卡等关键字段）；
  3. 制作**纠错规则**（例如功率缺失→置空；价格未识别→保留原图+Notes）。
* 成功判据：OCR 后**总体准确率≥90%**（或关键字段≥95%），且速度明显优于纯人工。

---

## 5) 纯人工备选方案（快速估时 & 分工）（30–60 分钟）

* **目标**：确保即使不能半自动，也能稳交。
* 动作：

  * 以**2000 条**估算：人工速度 ≈ **250–300 条/天**（视字段复杂度与截图量），**8–10 天**；
  * 制作**城市/省份分配表**（便于分工或并行），并准备**核验清单**。
* 成功判据：有清晰的人工作业计划，随时能启动。

---

## 6) 去重 & 统一口径（30–60 分钟）

* **目标**：避免重复、字段一致。
* 动作：

  * 定义唯一键：`Province + City + StationName + (Lat,Lng)`（经纬度优先）；
  * 做 **模糊去重**：名称清洗（去空格/全半角/常见后缀），地址规范化。
* 成功判据：重复率可检测与消除；终表唯一性良好。

---

## 7) 质量校验（QA）（30–60 分钟）

* **目标**：交付前的稳定性与可信度。
* 动作：

  * 抽样 5–10% 做**二次核验**（App/截图回看）；
  * 检查必填字段缺失率、坐标解析率、价格字段异常比例；
  * 输出一页**QA Summary**（问题项 & 解决策略 & 残留风险）。
* 成功判据：缺失/异常在可接受范围；QA报告可对外。

---

## 8) 交付打包与说明（30 分钟）

* **目标**：客户可直接验收。
* 动作：

  * 交付内容：

    * **主表**：CSV/Excel（字段按客户模板）；
    * **截图**：按`省/市/站点名_yyyymmdd.png`归档；
    * **方法说明**：1页简要说明（半自动/人工流程、字段口径、已知限制、QA摘要）。
* 成功判据：结构清晰、可追溯、有证据链（截图/来源路径）。

---

# 时间线建议（假设一周交付的半自动可行）

* Day 0–1：完成 0–4 步（验证接口/H5/OCR路线，确定最终方案），同时产出字段定义&命名规范；
* Day 2–5：批量采集 + 数据清洗 + 去重规范化；
* Day 6：QA 抽检 & 修复；
* Day 7：交付打包 & 简报。

> 若验证后判定**只能纯人工**：立即切换“人工作业计划”，并**第一时间同步客户**：将工期改为 **8–10 天**，给出具体里程碑（例如每两天提交一省/若干城市的增量数据）。

---

## 给客户的简短进度同步模板（可随时发送）

> Quick update: I validated nationwide access and heavy-truck filtering in the app. I’m now testing whether structured data can be extracted (API/H5), and in parallel preparing a fallback OCR pipeline. I’ll confirm within 24 hours whether semi-automation is feasible. If yes, I’ll deliver within one week; if not, I’ll proceed manually and deliver in 8–10 days with the same data quality standards.

---

需要的话，我可以按你电脑/手机的环境给你写一份**抓包与OCR的最小可行脚本骨架（Python）**，以及**QA抽检模板（Excel）**，让你直接开跑。
